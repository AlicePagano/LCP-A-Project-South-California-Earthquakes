{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Import main packages \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cmap\n",
    "from IPython.display import Image\n",
    "%matplotlib inline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Import packages for coordinate conversion\n",
    "import pyproj \n",
    "\n",
    "# Import packages for visualization \n",
    "import datetime\n",
    "import time\n",
    "from cartopy.mpl.ticker import LongitudeFormatter, LatitudeFormatter\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.io.img_tiles as cimgt\n",
    "import cartopy\n",
    "from mpl_toolkits import mplot3d\n",
    "\n",
    "# Import packages for time animation\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Import packages for animation\n",
    "import matplotlib.animation as animation\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Import useful packages\n",
    "import copy\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center> Earthquake Data Analysis \n",
    "\n",
    " **Michele Guadagnini** - 1230663 | **Alessandro Lambertini** - 1242885 \n",
    " \n",
    " **Alice Pagano** - 1236916 | **Michele Puppin** - 1227474\n",
    "\n",
    "\n",
    "In this project we consider a set of data representing earthquakes happened between the $1^{st}$ January 1982 and $28^{th}$ June 2011. The dataset is analyzed in order to compute the distribution of earthquakes in space and time and to visualize properly the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1 - Dataset description\n",
    "\n",
    "The dataset is organized in the following way: \n",
    "\n",
    "* The first column cointains the index of the event.\n",
    "* The second column contains the index of the previous event that triggered it (-1 if no ancestor is found).\n",
    "* The third column contains time in seconds from 0:00 of the $1^{st}$ January 1982 at which the earthquakes took place.\n",
    "* The fourth column corresponds to the magnitude of the earthquake in a range of [0,8].\n",
    "* Last three columns corresponds to the 3D  Euclidean coordinates in meters of the earthquake hypocenter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Read data from file and make sure earthquakes are ordered by time\n",
    "cols = ['index', 'trigger', 'time', 'magnitude', 'X', 'Y', 'Z']\n",
    "df = pd.read_table(\"SouthCalifornia-1982-2011_Physics-of-Data.dat\",sep=\" \",header=None)\n",
    "df.columns = cols\n",
    "df = df.sort_values(by = ['time'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In order to visualize earthquakes, we convert Euclidean coordinates of hypocenters (X, Y, Z) to geographic coordinates (longitude, latitude, depth) using the PyProj package. We consider a geographic coordinate system which uses a reference ellipsoid as an approximation of earth surface. In this case we use WGS-84 coordinate system, whose parameters are:\n",
    "\n",
    "* **Semi-major axis**: a = 6 378 137,000 000 m;\n",
    "\n",
    "* **Semi-minor axis**: b = 6 356 752,314 245 m;\n",
    "\n",
    "* **Eccentricity**: f = 1/298,257223563;\n",
    "\n",
    "<center>\n",
    "<img src='https://upload.wikimedia.org/wikipedia/commons/thumb/3/3e/WGS84_mean_Earth_radius.svg/440px-WGS84_mean_Earth_radius.svg.png' alt=\"\" width=\"\" height=\"\" style=\"\">\n",
    "<figcaption>Figure: Reppresentation of the reference ellipsoid in WGS-84. </figcaption>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The position with respect to the reference ellipsoid is given by: \n",
    "\n",
    "* longitude $\\lambda$: measures the rotational angle between the zero meridian and the measured point. By convention it is expressed in degrees ranging from $−180°$ to $+180°$.\n",
    "\n",
    "* latitude $\\phi$: measures how close to the poles or equator a point is along a meridian, and is represented as an angle from $−90°$ to $+90°$, where $0°$ is the equator. \n",
    "\n",
    "* depth $h$: ellipsoidal height of the point above or below the reference ellipsoid along its normal.\n",
    "\n",
    "If these coordinates are given, one can compute the Euclidean coordinates of the point as follows:\n",
    "\n",
    "\\begin{aligned}X&={\\big (}N(\\phi )+h{\\big )}\\cos {\\phi }\\cos {\\lambda }\\\\Y&={\\big (}N(\\phi )+h{\\big )}\\cos {\\phi }\\sin {\\lambda }\\\\Z&=\\left({\\frac {b^{2}}{a^{2}}}N(\\phi )+h\\right)\\sin {\\phi }\\end{aligned}\n",
    "where\n",
    "\n",
    "$$N(\\phi )= \\frac {a^{2}} {\\sqrt {a^{2}\\cos ^{2}\\phi +b^{2}\\sin ^{2}\\phi}}$$\n",
    "\n",
    "and $a$ and $b$ are the equatorial radius (semi-major axis) and the polar radius (semi-minor axis), respectively. $N$ is the radius of curvature in the prime vertical.\n",
    "\n",
    "<center>\n",
    "<img src='https://gssc.esa.int/navipedia/images/e/e8/Ellipsoidal_%26_Cartesian_Coord_Conv.png' alt=\"\" width=\"\" height=\"\" style=\"\">\n",
    "<figcaption>Figure: Reppresentation of Eucledian and geographic coordinates. </figcaption>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# With PyProj it is possible to tranform cartesian coordinates into geographic coordinates\n",
    "\n",
    "# Definition of the two reference frames \n",
    "ecef = pyproj.Proj(proj='geocent', ellps='WGS84', datum='WGS84') #euclidean\n",
    "lla  = pyproj.Proj(proj='latlong', ellps='WGS84', datum='WGS84') #latlon\n",
    "\n",
    "# Coordinates conversion\n",
    "points = np.array([df.X, df.Y, df.Z])\n",
    "gcoor  = pd.DataFrame(zip(*(pyproj.transform(ecef, lla, points[0], points[1], points[2], radians=False))))\n",
    "\n",
    "gcoor.columns = ['longitude', 'latitude', 'depth']\n",
    "df = pd.concat([df, gcoor], axis=1)\n",
    "\n",
    "display(df.head()) # Display dataset with new coordinates\n",
    "\n",
    "display(df.describe()) # Computing some statistics on the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2 - Visualization\n",
    "\n",
    "In order to visualize the process in time and space, we link to each earthquake the respective time in calendar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Add columns with the corresponding year and month\n",
    "offset = (datetime.datetime.fromisoformat('1982-01-01') - datetime.datetime.fromisoformat('1970-01-01')).total_seconds()\n",
    "\n",
    "df['total_second'] = df['time'] + offset + 8*60*60 \n",
    "\n",
    "year = np.empty(df['time'].values.shape[0], int)\n",
    "month = np.empty(df['time'].values.shape[0], int) \n",
    "day = np.empty(df['time'].values.shape[0], int) \n",
    "hour = np.empty(df['time'].values.shape[0], int) \n",
    "minute = np.empty(df['time'].values.shape[0], int) \n",
    "second = np.empty(df['time'].values.shape[0], int) \n",
    "\n",
    "for i in range(df['time'].values.shape[0]):\n",
    "    year[i] = datetime.datetime.fromtimestamp(df['total_second'].values[i]).year \n",
    "    month[i] = datetime.datetime.fromtimestamp(df['total_second'].values[i]).month \n",
    "    day[i] = datetime.datetime.fromtimestamp(df['total_second'].values[i]).day \n",
    "    hour[i] = datetime.datetime.fromtimestamp(df['total_second'].values[i]).hour\n",
    "    minute[i] = datetime.datetime.fromtimestamp(df['total_second'].values[i]).minute\n",
    "    second[i] = datetime.datetime.fromtimestamp(df['total_second'].values[i]).second\n",
    "    \n",
    "df['year'] = year\n",
    "df['month'] = month\n",
    "df['day'] = day\n",
    "df['hour'] = hour\n",
    "df['minute'] = minute\n",
    "df['second'] = second"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 2.1 - Tree plot\n",
    "\n",
    "Joining each event to that with the index of the second column (if not -1), there emerges a set of causal trees. First of all,  we count the number of event triggered by each earthquake in order to visualize efficently the causality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "num_trig = np.zeros(df.shape[0], dtype=int)\n",
    "\n",
    "counts = df.groupby('trigger')['index'].nunique() # Groupping elements with same trigger, selecting column index and counting the frequency of a trigger\n",
    "counts[-1] = 0  # Removing counts for events with no ancestor (trigger=-1)\n",
    "\n",
    "for i in counts.keys(): \n",
    "    num_trig[i] = counts[i]\n",
    "\n",
    "df['num_trig'] = np.array(num_trig)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We plot the total number of events and the average number of events triggered by earthquakes in different intervals of magnitude with step size of $0.1$. Observing the total number of triggered events, we notice that most of the earthquakes are triggered by events with small magnitude since they are much more. Instead, in the case of the average number of triggered events we deduce that, in relative terms, earthquakes with greater magnitude trigger a larger number of events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "interval_sum = np.arange(df['magnitude'].min(),df['magnitude'].max()+0.1,0.1) # Create the intervals of magnitude\n",
    "interval_mean = np.arange(df['magnitude'].min(),df['magnitude'].max()+0.1,0.1) # Create the intervals of magnitude\n",
    "\n",
    "number_sum = []\n",
    "number_mean = []\n",
    "\n",
    "for j in range(interval_sum.shape[0]-1): # Sum number of triggered events in the interval\n",
    "    df_mag = df[np.logical_and(df['magnitude']>=interval_sum[j], df['magnitude']<interval_sum[j+1])]\n",
    "    if df_mag.shape[0] != 0:\n",
    "        number_sum.append(df_mag['num_trig'].sum())\n",
    "    if df_mag.shape[0] == 0:\n",
    "        number_sum.append(np.nan)      \n",
    "        \n",
    "for j in range(interval_mean.shape[0]-1): # Compute the average number of triggered events in the interval\n",
    "    df_mag = df[np.logical_and(df['magnitude']>=interval_mean[j], df['magnitude']<interval_mean[j+1])]\n",
    "    number_mean.append(df_mag['num_trig'].mean(axis=0))\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "\n",
    "plt.title(\"Triggered events as a function of magnitude (1982-2011)\", fontsize=20)\n",
    "plt.xlabel('Magnitude', fontsize=14)\n",
    "plt.ylabel('Number of triggered events', fontsize=14)\n",
    "plt.plot(interval_sum[:-1],number_sum,color='red',linestyle='dashed',label='Total triggered number')\n",
    "plt.plot(interval_mean[:-1],number_mean,color='tab:blue',linestyle='dashdot',label='Average triggered number')  \n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We repeat a similar procedure considering depth intervals of $1 \\: km$ each. In this case we observe that shallowest earthquakes are the ones that trigger the larger number of events. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "interval_sum = np.arange(df['depth'].min(),df['depth'].max()+1000,1000) # Create the intervals of depth\n",
    "interval_mean = np.arange(df['depth'].min(),df['depth'].max()+1000,1000) # Create the intervals of depth\n",
    "\n",
    "number_sum = []\n",
    "number_mean = []\n",
    "\n",
    "for j in range(interval_sum.shape[0]-1): # Sum number of triggered events in the interval\n",
    "    df_dep = df[np.logical_and(df['depth']>=interval_sum[j], df['depth']<interval_sum[j+1])]\n",
    "    if df_dep.shape[0] != 0:\n",
    "        number_sum.append(df_dep['num_trig'].sum())\n",
    "    if df_dep.shape[0] == 0:\n",
    "        number_sum.append(np.nan)      \n",
    "        \n",
    "for j in range(interval_mean.shape[0]-1): # Compute the average number of triggered events in the interval\n",
    "    df_dep = df[np.logical_and(df['depth']>=interval_mean[j], df['depth']<interval_mean[j+1])]\n",
    "    number_mean.append(df_dep['num_trig'].mean(axis=0))\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(10, 8))\n",
    "color = 'tab:red'\n",
    "\n",
    "plt.title(\"Triggered events as a function of depth (1982-2011)\", fontsize=20)\n",
    "ax1.set_xlabel('Depth (m)', fontsize=14)\n",
    "ax1.set_ylabel('Total number of triggered events', fontsize=14,color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "ax1.plot(interval_sum[:-1],number_sum,color=color,linestyle='dashed',label='Total triggered number')\n",
    "\n",
    "ax2 = ax1.twinx() \n",
    "color = 'tab:blue'\n",
    "ax2.plot(interval_mean[:-1],number_mean,color=color,linestyle='dashdot',label='Average triggered number')  \n",
    "ax2.set_ylabel('Average number of triggered events', fontsize=14,color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "fig.tight_layout()  # Otherwise the right y-label is slightly clipped        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We produce a 3D plot of the number of triggered events in intervals of both magnitude and depth. As expected, we see that the largest number of events are triggered by shallow earthquakes with high magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "interval_mag = np.arange(2,8.1,0.1)  # Create the intervals of magnitude\n",
    "interval_dep = np.arange(df['depth'].min(),df['depth'].max()+1000,1000)  # Create the intervals of depth\n",
    "number = []\n",
    "x_grid = []\n",
    "y_grid = []\n",
    "\n",
    "for j in range(interval_mag.shape[0]-1): # Counting in interval of magnitude \n",
    "    for i in range(interval_dep.shape[0]-1): # Counting in interval of depth\n",
    "        df_m = df[np.logical_and(df['magnitude']>=interval_mag[j], df['magnitude']<interval_mag[j+1])]\n",
    "        df_d = df_m[np.logical_and(df_m['depth']>=interval_dep[i], df_m['depth']<interval_dep[i+1])]\n",
    "        number.append(df_d['num_trig'].sum())\n",
    "        x_grid.append(interval_mag[j])\n",
    "        y_grid.append(interval_dep[i])\n",
    "        \n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.set_xlabel('Magnitude', fontsize=14)\n",
    "ax.set_ylabel('Depth (m)', fontsize=14)\n",
    "ax.set_zlabel('Total number of triggered events', fontsize=14)\n",
    "ax.set_title(\"Triggered events as a function of depth (1982-2011)\", fontsize=20)\n",
    "ax.plot_trisurf(x_grid, y_grid, number,linewidth=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 2.2 - Plot of magnitude per month\n",
    "\n",
    "We produce an interactive plot of the magnitude of each earthquake in a given month and year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Fix the years and month of start and end of the animation\n",
    "START_YEAR = 1982\n",
    "END_YEAR = 2011\n",
    "START_MONTH = 1\n",
    "END_MONTH = 12\n",
    "\n",
    "# Setting the first frame\n",
    "m_max = df['magnitude'].max()\n",
    "m_min = df['magnitude'].min()\n",
    "points_slider = df[df['year']==START_YEAR] # Filter the dataframe for the starting year and month\n",
    " \n",
    "x_slider, y_slider = list(points_slider['month']), list(points_slider['magnitude']) # Select the longitude and magnitude column\n",
    "plt.close()\n",
    "\n",
    "df_slider = df\n",
    "def update1(year_slider): # Update the plot for the selected year\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.ylim((m_min-0.1,m_max+0.1))\n",
    "    plt.xlim(0,13)\n",
    "    plt.xticks([i for i in range(1,13)])\n",
    "    current_year = year_slider\n",
    "    points_slider = df_slider[df_slider['year']==current_year]\n",
    "    x_slider, y_slider = list(points_slider['month']), list(points_slider['magnitude'])\n",
    "    plt.title(\"Magnitude earthquake visualisation (\"+str(START_YEAR)+'-'+str(END_YEAR)+')', fontsize=20)\n",
    "    plt.xlabel('Month', fontsize=14)\n",
    "    plt.ylabel('Magnitude', fontsize=14)\n",
    "    plt.plot(x_slider,y_slider,'.',color='red')\n",
    "    fig.canvas.draw_idle()\n",
    "\n",
    "anim = interact(update1, year_slider = widgets.IntSlider(value=1992, min=START_YEAR, max=END_YEAR, step=1));\n",
    "\n",
    "anim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 2.3 - Visualization in space and time\n",
    "\n",
    "Eventually, we create an animation to visualize in space and time earthquakes plotted on a 2D map. In each frame we represent all earthquakes happened in a fixed month and year with a scatter plot where color of the dots represents depth and its size represents the magnitude. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# CREATE THE ANIMATION\n",
    "fig = plt.figure(figsize=(12, 10))\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "\n",
    "# Formatting axes ticks\n",
    "ax.set_xticks([-114, -116, -118, -120, -122, -124], crs=ccrs.PlateCarree())\n",
    "ax.set_yticks([30, 32, 34, 36, 38], crs=ccrs.PlateCarree())\n",
    "lon_formatter = LongitudeFormatter(zero_direction_label=True)\n",
    "lat_formatter = LatitudeFormatter()\n",
    "ax.xaxis.set_major_formatter(lon_formatter)\n",
    "ax.yaxis.set_major_formatter(lat_formatter)\n",
    "extent = [-125.97, -111.95, 28.62, 38.41]\n",
    "ax.set_extent(extent)\n",
    "\n",
    "# Downloading background from Stamen Maps \n",
    "stamen_terrain = cimgt.Stamen('terrain-background')\n",
    "ax.add_image(stamen_terrain, 7)   \n",
    "  \n",
    "# Plot the position of some cities on the map\n",
    "plt.plot(-122.4194200,  37.7749300, '+', ms=8, color = 'black') # San Francisco position\n",
    "SFlabel = plt.text(-124, 38, 'San Francisco', fontsize=12, color='black')\n",
    "plt.plot(-115.172813, 36.114647, '+', ms=8, color = 'black')  # Las Vegas position\n",
    "SFlabel = plt.text(-114.8, 36.2, 'Las Vegas', fontsize=12, color='black')\n",
    "plt.plot(-112.074036, 33.448376, '+', ms=8, color = 'black') # Phoenix position\n",
    "SFlabel = plt.text(-113.2, 33.6, 'Phoenix', fontsize=12, color='black')\n",
    "plt.plot(-118.243683, 34.052235, '+', ms=8, color = 'black')  # Los Angeles position\n",
    "SFlabel = plt.text(-120.1, 33.8, 'Los Angeles', fontsize=12, color='black')  \n",
    "\n",
    "# Fix the years and month of start and end of the animation\n",
    "START_YEAR = 1982\n",
    "END_YEAR = 2011\n",
    "START_MONTH = 1\n",
    "END_MONTH = 12\n",
    "\n",
    "# Setting the first frame\n",
    "# Filter the dataframe for the starting year and month\n",
    "points = df[df['year']==START_YEAR]\n",
    "points = points[points['month']==START_MONTH]\n",
    "x, y = list(points['longitude']), list(points['latitude']) # Select the longitude and magnitude column\n",
    "mag = np.array((np.exp(1.15*points['magnitude']))) # Select the magnitude column passing the exponent of the value for highilighting the difference\n",
    "color = np.array(points['depth']/1000.) # Select the depth column expressing it in km\n",
    "\n",
    "# Create the scatter plot, where the magntitude scales as the radius and the depth as the color \n",
    "scat = plt.scatter(x, y, s = mag,  marker='o', alpha=0.7, c=color, cmap='hot', vmin=df['depth'].min()/1000.0, vmax=df['depth'].max()/1000.0)  \n",
    "\n",
    "# Create legend of the magntitude\n",
    "for magn in [2, 3, 4, 5, 6]:\n",
    "    plt.scatter([], [], c='black', alpha=0.7, s=np.exp(1.15*magn), label=str(magn))   \n",
    "leg = plt.legend(scatterpoints=1, frameon=False, labelspacing=2, title='Magnitude', loc=\"upper left\")\n",
    "plt.setp(leg.get_title(), color='black', fontsize=12)\n",
    "plt.setp(leg.get_texts(), color='black')\n",
    "\n",
    "# Create legend of the depth\n",
    "cbar = plt.colorbar(scat, shrink = 0.9)\n",
    "cbar.set_label('Depth [km]', fontsize=14)\n",
    "\n",
    "plt.title(\"Earthquake visualisation (\"+str(START_YEAR)+'-'+str(END_YEAR)+')', fontsize=20) #title\n",
    "year_text = plt.text(-125.5, 29., str(START_MONTH)+'-'+str(START_YEAR), fontsize=30, color='black') #month and year of the frame\n",
    "plt.axis(aspect='equal')\n",
    "plt.xlabel('Longitude', fontsize=14)\n",
    "plt.ylabel('Latitude', fontsize=14)\n",
    "plt.close()\n",
    "\n",
    "def update(frame_number): # Update the plot for increasing year and month\n",
    "    current_year = START_YEAR + int(((frame_number/12) % (END_YEAR - START_YEAR + 1)))\n",
    "    current_month = START_MONTH + (frame_number % 12)\n",
    "    year_text.set_text(str(current_month)+'-'+str(current_year)) \n",
    "    points = df[df['year']==current_year]\n",
    "    points = points[points['month']==current_month]\n",
    "    x, y = list(points['longitude']), list(points['latitude'])\n",
    "    mag = np.array( (np.exp(1.15*points['magnitude'])) )\n",
    "    color = np.array(points['depth']/1000.)\n",
    "    scat.set_offsets(np.c_[x,y]) \n",
    "    scat.set_sizes(mag)\n",
    "    scat.set_array(color)\n",
    "    \n",
    "ani = animation.FuncAnimation(fig, update, interval=0.0001, frames=12*(END_YEAR - START_YEAR +1))\n",
    "ani.save('animation.gif', writer='imagemagick', fps=3)    \n",
    "filename = 'animation.gif'\n",
    "video = io.open(filename, 'r+b').read()\n",
    "encoded = base64.b64encode(video)\n",
    "HTML(data='''<img src=\"data:image/gif;base64,{0}\" type=\"gif\" />'''.format(encoded.decode('ascii')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Power Law Distribution\n",
    "\n",
    "Seismicity is a complex spatiotemporal phenomenon which obeys certain simple general laws that govern the statistics of their occurence. In particular, waiting time and distance of earthquakes is expected to follow a power law distribution:\n",
    "\n",
    "$$ P(t) = C t^{- \\alpha} $$\n",
    "\n",
    "A power law distribuion is visualized as a straight line on a log-log plot:\n",
    "\n",
    "$$ \\ln P(t) = \\ln C - \\alpha \\ln t $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3 - Distribution of waiting time $P_m(t)$\n",
    "\n",
    "We compute the distribution of waiting time for earthquakes of magnitude m or above for m in [2,5]. For a fixed m we compute the waiting time as the difference between the time of an event and the previous one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Read data from file and make sure earthquakes are ordered by time\n",
    "cols = ['index', 'trigger', 'time', 'magnitude', 'X', 'Y', 'Z']\n",
    "df = pd.read_table(\"SouthCalifornia-1982-2011_Physics-of-Data.dat\",sep=\" \",header=None)\n",
    "df.columns = cols\n",
    "df = df.sort_values(by = ['time'])\n",
    "\n",
    "m = 3 # Set lower bound to magnitude\n",
    "\n",
    "df_red = copy.deepcopy(df[df['magnitude']>=m]) # Filter earthquakes with magnitude m o above\n",
    "\n",
    "print(\"Number of selected earthquakes: \", df_red.shape[0])\n",
    "\n",
    "df_red['waiting'] = df_red['time'].diff() # Compute the waiting time \n",
    "df_red.loc[df_red.idxmin(),['waiting']] = 0. # Set the first waiting time to 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 3.1 - Histogram with regular binning\n",
    "\n",
    "First of all we plot the histogram of normalized frequency vs waiting time using a fixed binning proportional to the $\\sqrt{N}$ where $N$ is the number of total events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Plot a normalized histogram\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "ax = sns.distplot(df_red['waiting'], bins=10*int(np.sqrt(df_red.shape[0])), kde=False, norm_hist=True)\n",
    "ax.set_xlim(-10000,500000)\n",
    "ax.set_title('Waiting time histogram with regular binning')\n",
    "ax.set_xlabel('Waiting time [s]')\n",
    "ax.set_ylabel('Normalized Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In order to improve visualization, we plot the dotted histogram in log-log scale. We notice some noise in the tail of the distribution due to the low number of entries in that region. The result is that we have more information on the part of the distribution we care the less. As said, assuming power law behaviour, we should perform a linear regression on data in log-log scale histogram to extrapulate the parameter of the distribution. However, the noise introduce a bias on the slope. In order to erase the noise, we need to enlarge bin size in the tail of the distribution. Usually a logarithmic binnins is used for this scope. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Copy data from histogram to produce a dotted histogram\n",
    "width = np.array([h.get_width() for h in ax.patches]) \n",
    "edge = np.array([h.get_xy()[0] for h in ax.patches]) # Left edges\n",
    "x = np.add(edge,width/2.) # Center of bins\n",
    "y = [h.get_height() for h in ax.patches] # Frequency of data \n",
    "\n",
    "fig = plt.subplots(figsize=(10,8))\n",
    "plt.plot(x,y,'.')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.title('Waiting time histogram with regular binning in log-log scale')\n",
    "plt.xlabel('Waiting time [s]')\n",
    "plt.ylabel('Normalized Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 3.2 - Histogram with logarithmic binning\n",
    "\n",
    "We compute the histogram with logarithmic binning and we observe in the dotted log-log scale histogram that the noise has been removed. Now the information is more uniformly distributed among bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Set logarithmic binning\n",
    "\n",
    "width = 1000*np.logspace(0, 3, int(np.sqrt(df_red.shape[0]))+1)\n",
    "edge = np.cumsum(width) # Compute the right edges\n",
    "\n",
    "# Histogram with logarithmic binning\n",
    "ax1 = sns.distplot(df_red['waiting'], bins=edge, kde=False) # Non-normalized plot used to compute errors\n",
    "plt.close()\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "ax = sns.distplot(df_red['waiting'], bins=edge, kde=False, norm_hist=True) # Plot a normalized histogram\n",
    "\n",
    "ax.set_xlim(-100000,1000000)\n",
    "ax.set_title('Waiting time histogram with logarithmic binning')\n",
    "ax.set_xlabel('Waiting time [s]')\n",
    "ax.set_ylabel('Normalized Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We notice a bending in the distibution which suggests that the head of the distribution behaves as a power law while the tail decays more rapidly (i.e. as an exponential) since events with large waiting times are less likely to happen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Compute the error of the bin as a Poissonian\n",
    "width = np.array([h.get_width() for h in ax1.patches])\n",
    "y1 = np.array([h.get_height() for h in ax1.patches]) # Frequency of data \n",
    "y_error = np.divide(np.sqrt(y1),np.sum(y1)*width)\n",
    "\n",
    "# Copy data from histogram to produce a dot histogram\n",
    "width = np.array([h.get_width() for h in ax.patches]) \n",
    "edge = np.array([h.get_xy()[0] for h in ax.patches]) # Left edges\n",
    "x = np.add(edge,width/2.) # Center of bins\n",
    "y = [h.get_height() for h in ax.patches] # Frequency of data \n",
    "\n",
    "fig = plt.subplots(figsize=(10,8))\n",
    "plt.errorbar(x, y,yerr=y_error,fmt='.')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.title('Waiting time histogram with logarithmic binning in log-log scale')\n",
    "plt.xlabel('Waiting time [s]')\n",
    "plt.ylabel('Normalized Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 3.3 - Linear regression with tuning of lower cutoff\n",
    "\n",
    "In order to estimate the exponent of the power law decay we need to consider only the linear part of the distribution. Hence, we fix a lower bound to stop at the last bin before the bending begin. \n",
    "We consider the set of arrays each containing the points of the histogram from the first to the i-th with i that goes from the first to the last point. The i-th point corresponds to the lower bound. We perform linear regression for each of these arrays and we plot the slope vs the lower bound of the fitted array. Eventually we choose as lower bound a point in the region of stability of the value of the slope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Compute vectors with logarithm of frequences and centers of edges in order to perform linear regression\n",
    "xy = pd.DataFrame({'x':x, 'y':y})\n",
    "xy = xy[ xy.y != 0 ] # Drop bin with zero frequency in order to compute the logarithm\n",
    "x_reg = np.log10(np.array(xy.x))\n",
    "y_reg = np.log10(np.array(xy.y))\n",
    "\n",
    "alpha = []\n",
    "x_part = []\n",
    "\n",
    "for i in range(3,len(x_reg)): # Perfom linear regression after removing elements from x_m to the end of the vector\n",
    "    x_prov = x_reg[:i]\n",
    "    y_prov = y_reg[:i]\n",
    "    model = LinearRegression().fit(x_prov.reshape(-1,1), y_prov.reshape(-1,1))\n",
    "    alpha.append(model.coef_[0][0])\n",
    "    x_part.append(i)\n",
    "\n",
    "fig = plt.subplots(figsize=(10,10))\n",
    "plt.plot(x_part,alpha,'.')\n",
    "plt.title('Plot for stability of the slope')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Slope [s$^{-1}$]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Once we chose the lower bound, we perform linear regression and we observe values of the slope, for all m, in a range between about 0.65 and 1, which are similar to values we found in litterature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Choose best lower bound\n",
    "slope = []\n",
    "chosen_index = []\n",
    "\n",
    "for i in range(len(x_part)-10): # Compute the slope for a set of 10 consecutive points\n",
    "    x_set = np.array(x_part[i:i+10])\n",
    "    y_set = np.array(alpha[i:i+10])\n",
    "    model = LinearRegression().fit(x_set.reshape(-1,1), y_set.reshape(-1,1))\n",
    "    chosen_index.append(i+10)\n",
    "    slope.append(model.coef_[0][0])\n",
    "\n",
    "data_tuples = list(zip(chosen_index,slope))\n",
    "g = pd.DataFrame(data_tuples,columns=['x_min_index','slope'])\n",
    "g['slope'] = np.abs(g['slope'])\n",
    "x_min_index =  np.array(g.loc[g['slope'] == g['slope'].min()]) # Choose the last point of the set as lower bound\n",
    "\n",
    "# Perform linear regression with the best value x_min\n",
    "x_corr = np.array(x_reg[1:int(x_min_index[0][0])])\n",
    "y_corr = np.array(y_reg[1:int(x_min_index[0][0])])\n",
    "\n",
    "model = LinearRegression().fit(x_corr.reshape(-1,1), y_corr.reshape(-1,1))\n",
    "\n",
    "print('Intercept:', model.intercept_)\n",
    "print('Slope:', model.coef_)\n",
    "\n",
    "y_error_corr = np.divide(y_error[1:int(x_min_index[0][0])],y[1:int(x_min_index[0][0])]) # Compute errors\n",
    "\n",
    "# Plot fit\n",
    "l = np.linspace(x_corr.min(), x_corr.max(),len(x_corr))\n",
    "\n",
    "fig = plt.subplots(figsize=(10,8))\n",
    "plt.errorbar(x_corr, y_corr,yerr=y_error_corr,fmt='.')\n",
    "plt.plot(l, model.intercept_[0] + model.coef_[0][0]*l)\n",
    "plt.title('Linear regression for the power law range')\n",
    "plt.xlabel('Log$_{10}$(Waiting time [s])')\n",
    "plt.ylabel('Log$_{10}$(Normalized Frequency)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 3.4 - Analysis for different values of $m$\n",
    "\n",
    "Eventually we perform this analysis for different values of $m$ and show the result in the same plot for comparison. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "mag = [2,3,4,5] # Set the value of m\n",
    "xx_waiting = [0,0,0,0]\n",
    "yy_waiting = [0,0,0,0]\n",
    "w_mean = [0,0,0,0]\n",
    "\n",
    "for i in range(len(mag)): # Repeat the analysis for m in mag\n",
    "    df_red = copy.deepcopy(df[df['magnitude'] >= mag[i]])\n",
    "    df_red['waiting'] = df_red['time'].diff()\n",
    "    df_red.loc[df_red.idxmin(),['waiting']] = 0.\n",
    "    w_mean[i] = df_red['waiting'].mean()\n",
    "    width = 1000*np.logspace(0, 3, int(np.sqrt(df_red.shape[0]))+1)\n",
    "    edge = np.cumsum(width)\n",
    "    fig, ax = plt.subplots(figsize=(10,8))\n",
    "    ax = sns.distplot(df_red['waiting'], bins = edge, kde=False, norm_hist=True)\n",
    "    edge = np.array([h.get_xy()[0] for h in ax.patches]) \n",
    "    width = np.array([h.get_width() for h in ax.patches])\n",
    "    xx_waiting[i] = np.add(edge, -width/2.)\n",
    "    yy_waiting[i] = np.array([h.get_height() for h in ax.patches])\n",
    "    plt.close()\n",
    "\n",
    "# Plot for all m in log-log scale\n",
    "fig = plt.subplots(figsize=(10,8))\n",
    "for i in range(len(mag)):\n",
    "    plt.plot(xx_waiting[i],yy_waiting[i],'.',label=\"m: {}\".format(i+2))\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.title('Histogram with logarithmic binning in log-log scale')\n",
    "plt.xlabel('Waiting time [s]')\n",
    "plt.ylabel('Normalized Frequency')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4 - Distribution of distances $P_m(r)$\n",
    "\n",
    "We compute the distribution $P_m(r)$ of the distance between hypocenters of an event and the next one in Euclidian coordinates, considering earthquakes of magnitude m or above. Since we assume that distances are distributed as power law as well, we repeat the whole procedure. Again we plot results for different values of $m$ in the same plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Read data from file and make sure earthquakes are ordered by time\n",
    "cols = ['index', 'trigger', 'time', 'magnitude', 'X', 'Y', 'Z']\n",
    "df = pd.read_table(\"SouthCalifornia-1982-2011_Physics-of-Data.dat\",sep=\" \",header=None)\n",
    "df.columns = cols\n",
    "df = df.sort_values(by = ['time'])\n",
    "\n",
    "m = 3 # Set lower bound to magnitude\n",
    "\n",
    "df_red = copy.deepcopy(df[df['magnitude']>=m]) # Filter earthquakes with magnitude m o above\n",
    "\n",
    "print(\"Number of selected earthquakes: \", df_red.shape[0])\n",
    "\n",
    "df_red['distance'] = np.sqrt((df_red['X'].diff(periods=-1))**2 + (df_red['Y'].diff(periods=-1))**2 + (df_red['Z'].diff(periods=-1))**2)\n",
    "df_red = df_red.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 4.1 - Histogram with regular binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Plot a normalized histogram\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "ax = sns.distplot(df_red['distance'], bins=10*int(np.sqrt(df_red.shape[0])), kde=False, norm_hist=True)\n",
    "ax.set_title(\"Distances' histogram with regular binning\")\n",
    "ax.set_xlabel('Distance [m]')\n",
    "ax.set_ylabel('Normalized frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Copy data from histogram to produce a dot histogram\n",
    "width = np.array([h.get_width() for h in ax.patches]) \n",
    "edge = np.array([h.get_xy()[0] for h in ax.patches]) # Left edges\n",
    "x = np.add(edge,width/2.) # Center of bins\n",
    "y = [h.get_height() for h in ax.patches] # Frequency of data \n",
    "\n",
    "fig = plt.subplots(figsize=(10,8))\n",
    "plt.plot(x,y,'.')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.title(\"Distances' histogram with regular binning in log-log scale\")\n",
    "plt.xlabel('Distance [m]')\n",
    "plt.ylabel('Normalized frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 4.2 - Histogram with logarithmic binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Set logarithmic binning\n",
    "\n",
    "width = 1000*np.logspace(0, 3, int(np.sqrt(df_red.shape[0]))+1)\n",
    "edge = np.cumsum(width) # Compute the right edges\n",
    "\n",
    "# Histogram with logarithmic binning\n",
    "ax1 = sns.distplot(df_red['distance'], bins=edge, kde=False) # Non-normalized plot used to compute errors\n",
    "plt.close()\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "ax = sns.distplot(df_red['distance'], bins=edge, kde=False, norm_hist=True) # Plot a normalized histogram\n",
    "\n",
    "ax.set_xlim(-10000,1000000)\n",
    "ax.set_title(\"Distances' histogram with logarithmic binning\")\n",
    "ax.set_xlabel('Distance [m]')\n",
    "ax.set_ylabel('Normalized frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Compute the error of the bin as a Poissonian\n",
    "width = np.array([h.get_width() for h in ax1.patches])\n",
    "y1 = np.array([h.get_height() for h in ax1.patches]) # Frequency of data \n",
    "y_error = np.divide(np.sqrt(y1),np.sum(y1)*width)\n",
    "\n",
    "# Copy data from histogram to produce a dot histogram\n",
    "width = np.array([h.get_width() for h in ax.patches]) \n",
    "edge = np.array([h.get_xy()[0] for h in ax.patches]) # Left edges\n",
    "x = np.add(edge,width/2.) # Center of bins\n",
    "y = [h.get_height() for h in ax.patches] # Frequency of data \n",
    "\n",
    "fig = plt.subplots(figsize=(10,8))\n",
    "plt.errorbar(x, y,yerr=y_error,fmt='.')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.title(\"Distances' histogram with logarithmic binning in log-log scale\")\n",
    "plt.xlabel('Distance [m]')\n",
    "plt.ylabel('Normalized frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 4.3 - Linear regression with tuning of lower cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Compute vectors with logarithm of frequences and centers of edges in order to perform linear regression\n",
    "xy = pd.DataFrame({'x':x, 'y':y})\n",
    "xy = xy[ xy.y != 0 ] # Drop bin with zero frequency in order to compute the logarithm\n",
    "x_reg = np.log10(np.array(xy.x))\n",
    "y_reg = np.log10(np.array(xy.y))\n",
    "\n",
    "# Perfom linear regression after removing elements from x_m to the end of the vector\n",
    "alpha = []\n",
    "x_part = []\n",
    "\n",
    "for i in range(3,len(x_reg)):\n",
    "    x_prov = x_reg[:i]\n",
    "    y_prov = y_reg[:i]\n",
    "    model = LinearRegression().fit(x_prov.reshape(-1,1), y_prov.reshape(-1,1))\n",
    "    alpha.append(model.coef_[0][0])\n",
    "    x_part.append(i)\n",
    "\n",
    "fig = plt.subplots(figsize=(10,8))\n",
    "plt.plot(x_part,alpha,'.')\n",
    "plt.title('Plot for stability of the slope')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Slope [m$^{-1}$]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Choose best lower bound\n",
    "slope = []\n",
    "chosen_index = []\n",
    "\n",
    "for i in range(len(x_part)-5): # Compute the slope for al consecutive set of five points\n",
    "    x_set = np.array(x_part[i:i+5])\n",
    "    y_set = np.array(alpha[i:i+5])\n",
    "    model = LinearRegression().fit(x_set.reshape(-1,1), y_set.reshape(-1,1))\n",
    "    chosen_index.append(i+5)\n",
    "    slope.append(model.coef_[0][0])\n",
    "\n",
    "data_tuples = list(zip(chosen_index,slope))\n",
    "g = pd.DataFrame(data_tuples,columns=['x_min_index','slope'])\n",
    "g['slope'] = np.abs(g['slope'])\n",
    "x_min_index =  np.array(g.loc[g['slope'] == g['slope'].min()]) # Choose the last point of the set with lower bound\n",
    "\n",
    "# Perform linear regression with the best value x_min\n",
    "x_corr = np.array(x_reg[1:int(x_min_index[0][0])])\n",
    "y_corr = np.array(y_reg[1:int(x_min_index[0][0])])\n",
    "\n",
    "model = LinearRegression().fit(x_corr.reshape(-1,1), y_corr.reshape(-1,1))\n",
    "\n",
    "print('Intercept:', model.intercept_)\n",
    "print('Slope:', model.coef_)\n",
    "\n",
    "y_error_corr = np.divide(y_error[1:int(x_min_index[0][0])],y[1:int(x_min_index[0][0])]) # Compute errors\n",
    "\n",
    "# Plot fit\n",
    "l = np.linspace(x_corr.min(), x_corr.max(),len(x_corr))\n",
    "\n",
    "fig = plt.subplots(figsize=(10,8))\n",
    "plt.errorbar(x_corr, y_corr,yerr=y_error_corr,fmt='.')\n",
    "plt.plot(l, model.intercept_[0] + model.coef_[0][0]*l)\n",
    "plt.title('Linear regression for the power low range')\n",
    "plt.xlabel('Log$_{10}$(Distance [m])')\n",
    "plt.ylabel('Log$_{10}$(Normalized frequency)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 4.4 - Analysis for different values of $m$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "mag = [2,3,4,5] # Set varius the value of m\n",
    "xx_dist = [0,0,0,0]\n",
    "yy_dist = [0,0,0,0]\n",
    "mean = [0,0,0,0]\n",
    "max_dist = [0,0,0,0]\n",
    "\n",
    "for i in range(len(mag)):\n",
    "    df_red = copy.deepcopy(df[df['magnitude'] >= mag[i]])\n",
    "    df_red['distance'] = np.sqrt((df_red['X'].diff(periods=-1))**2 + (df_red['Y'].diff(periods=-1))**2 + \n",
    "                                 (df_red['Z'].diff(periods=-1))**2)\n",
    "    df_red = df_red.dropna()\n",
    "    max_dist[i] = df_red['distance'].max()\n",
    "    width = 1000*np.logspace(0, 3, int(np.sqrt(df_red.shape[0]))+1)\n",
    "    edge = np.cumsum(width)\n",
    "    fig, ax = plt.subplots(figsize=(10,8))\n",
    "    ax = sns.distplot(df_red['distance'], bins = edge, kde=False, norm_hist=True)\n",
    "    edge = np.array([h.get_xy()[0] for h in ax.patches]) \n",
    "    width = np.array([h.get_width() for h in ax.patches])\n",
    "    xx_dist[i] = np.add(edge, -width/2.)\n",
    "    yy_dist[i] = np.array([h.get_height() for h in ax.patches])\n",
    "    plt.close()\n",
    "\n",
    "# Plot for all m in log-log scale\n",
    "\n",
    "fig = plt.subplots(figsize=(10,8))\n",
    "\n",
    "for i in range(len(mag)):\n",
    "    plt.plot(xx_dist[i],yy_dist[i],'.',label=\"m: {}\".format(i+2))\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.title('Histograms with logarithmic binning in log-log scale')\n",
    "plt.xlabel('Distance [m]')\n",
    "plt.ylabel('Normalized frequency')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 5 - Distribution of waiting time below a certain distance $P_{m,R}(t)$\n",
    "\n",
    "We compute the distribution $P_{m,R}(t)$ of waiting times for events of magnitude $m$ or above, which are separated by at most a distance $r<R$, for different values of m and $R$. We perform a similar analysis as above for $m$ in [2,3,4,5] and $R$ in $[R^* /2, R^*,2R^*]$ where $R^*$ is the mean value of the distance for a fixed $m$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Read data from file and make sure earthquakes are ordered by time\n",
    "cols = ['index', 'trigger', 'time', 'magnitude', 'X', 'Y', 'Z']\n",
    "df = pd.read_table(\"SouthCalifornia-1982-2011_Physics-of-Data.dat\",sep=\" \",header=None)\n",
    "df.columns = cols\n",
    "df = df.sort_values(by = ['time'])\n",
    "\n",
    "m = 3 # Set lower bound to magnitude\n",
    "\n",
    "df_m = copy.deepcopy(df[df['magnitude'] >= m]) # Filter earthquakes with magnitude m o above\n",
    "\n",
    "print(\"Number of selected earthquakes: \", df_m.shape[0])\n",
    "\n",
    "df_m['distance'] = np.sqrt(( df_m['X'].diff(periods=-1) )**2+( df_m['Y'].diff(periods=-1) )**2+\n",
    "                           ( df_m['Z'].diff(periods=-1) )**2)\n",
    "df_m = df_m.dropna()\n",
    "r = df_m['distance'].mean()\n",
    "df_r = copy.deepcopy(df_m[df_m['distance']<r])\n",
    "\n",
    "# Compute the waiting time and and set the first waiting time to 0\n",
    "df_r['waiting'] = df_r['time'].diff()\n",
    "df_r.loc[df_r.idxmin(),['waiting']] = 0.\n",
    "df_red = copy.deepcopy(df_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 5.1 - Histogram with regular binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Plot a normalized histogram\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "ax = sns.distplot(df_red['waiting'], bins=10*int(np.sqrt(df_red.shape[0])), kde=False, norm_hist=True)\n",
    "ax.set_xlim(-10000,600000)\n",
    "ax.set_title('Waiting time histogram filtered by distance with regular binning')\n",
    "ax.set_xlabel('Waiting time [s]')\n",
    "ax.set_ylabel('Normalized frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Copy data from histogram to produce a dot histogram\n",
    "width = np.array([h.get_width() for h in ax.patches]) \n",
    "edge = np.array([h.get_xy()[0] for h in ax.patches]) # Left edges\n",
    "x = np.add(edge,width/2.) # Center of bins\n",
    "y = [h.get_height() for h in ax.patches] # Frequency of data \n",
    "\n",
    "fig = plt.subplots(figsize=(10,8))\n",
    "plt.plot(x,y,'.')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.title('Waiting time histogram filtered by distance with regular binning in log-log scale')\n",
    "plt.xlabel('Waiting time [s]')\n",
    "plt.ylabel('Normalized frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 5.2 - Histogram with logarithmic binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Set logarithmic binning\n",
    "\n",
    "width = 1000*np.logspace(0, 3, int(np.sqrt(df_red.shape[0]))+1)\n",
    "edge = np.cumsum(width) # Compute the right edges\n",
    "\n",
    "# Histogram with logarithmic binning\n",
    "ax1 = sns.distplot(df_red['waiting'], bins=edge, kde=False) # Non-normalized plot used to compute errors\n",
    "plt.close()\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "ax = sns.distplot(df_red['waiting'], bins=edge, kde=False, norm_hist=True) # Plot a normalized histogram\n",
    "\n",
    "ax.set_xlim(-10000,600000)\n",
    "ax.set_title('Waiting time histogram filtered by distance with logarithmic binning')\n",
    "ax.set_xlabel('Waiting time [s]')\n",
    "ax.set_ylabel('Normalized frequency')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Compute the error of the bin as a Poissonian\n",
    "width = np.array([h.get_width() for h in ax1.patches])\n",
    "y1 = np.array([h.get_height() for h in ax1.patches]) # Frequency of data \n",
    "y_error = np.divide(np.sqrt(y1),np.sum(y1)*width)\n",
    "\n",
    "# Copy data from histogram to produce a dot histogram\n",
    "width = np.array([h.get_width() for h in ax.patches]) \n",
    "edge = np.array([h.get_xy()[0] for h in ax.patches]) # Left edges\n",
    "x = np.add(edge,width/2.) # Center of bins\n",
    "y = [h.get_height() for h in ax.patches] # Frequency of data \n",
    "\n",
    "fig = plt.subplots(figsize=(10,10))\n",
    "plt.errorbar(x, y,yerr=y_error,fmt='.')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.title('Waiting time histogram filtered by distance with logarithmic binning in log-log scale')\n",
    "plt.xlabel('Waiting time [s]')\n",
    "plt.ylabel('Normalized frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 5.3 - Linear regression with tuning of lower cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Compute vectors with logarithm of frequences and centers of edges in order to perform linear regression\n",
    "xy = pd.DataFrame({'x':x, 'y':y})\n",
    "xy = xy[ xy.y != 0 ] # Drop bin with zero frequency in order to compute the logarithm\n",
    "x_reg = np.log10(np.array(xy.x))\n",
    "y_reg = np.log10(np.array(xy.y))\n",
    "\n",
    "# Perfom linear regression after removing elements from x_m to the end of the vector\n",
    "alpha = []\n",
    "x_part = []\n",
    "\n",
    "for i in range(3,len(x_reg)):\n",
    "    x_prov = x_reg[:i]\n",
    "    y_prov = y_reg[:i]\n",
    "    model = LinearRegression().fit(x_prov.reshape(-1,1), y_prov.reshape(-1,1))\n",
    "    alpha.append(model.coef_[0][0])\n",
    "    x_part.append(i)\n",
    "\n",
    "fig = plt.subplots(figsize=(10,10))\n",
    "plt.plot(x_part,alpha,'.')\n",
    "plt.title('Plot for stability of the slope')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Slope [s$^{-1}$]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Choose best lower bound\n",
    "slope = []\n",
    "chosen_index = []\n",
    "\n",
    "for i in range(len(x_part)-5): # Compute the slope for al consecutive set of five points\n",
    "    x_set = np.array(x_part[i:i+5])\n",
    "    y_set = np.array(alpha[i:i+5])\n",
    "    model = LinearRegression().fit(x_set.reshape(-1,1), y_set.reshape(-1,1))\n",
    "    chosen_index.append(i+5)\n",
    "    slope.append(model.coef_[0][0])\n",
    "\n",
    "data_tuples = list(zip(chosen_index,slope))\n",
    "g = pd.DataFrame(data_tuples,columns=['x_min_index','slope'])\n",
    "g['slope'] = np.abs(g['slope'])\n",
    "x_min_index =  np.array(g.loc[g['slope'] == g['slope'].min()]) # Choose the last point of the set with lower bound\n",
    "\n",
    "# Perform linear regression with the best value x_min\n",
    "x_corr = np.array(x_reg[1:int(x_min_index[0][0])])\n",
    "y_corr = np.array(y_reg[1:int(x_min_index[0][0])])\n",
    "\n",
    "model = LinearRegression().fit(x_corr.reshape(-1,1), y_corr.reshape(-1,1))\n",
    "\n",
    "print('Intercept:', model.intercept_)\n",
    "print('Slope:', model.coef_)\n",
    "\n",
    "y_error_corr = np.divide(y_error[1:int(x_min_index[0][0])],y[1:int(x_min_index[0][0])]) # Compute errors\n",
    "\n",
    "# Plot fit\n",
    "l = np.linspace(x_corr.min(), x_corr.max(),len(x_corr))\n",
    "\n",
    "fig = plt.subplots(figsize=(10,10))\n",
    "plt.errorbar(x_corr, y_corr,yerr=y_error_corr,fmt='.')\n",
    "plt.plot(l, model.intercept_[0] + model.coef_[0][0]*l)\n",
    "plt.title('Linear regression for the power law range')\n",
    "plt.xlabel('Log$_{10}$(Waiting time [s])')\n",
    "plt.ylabel('Log$_{10}$(Normalized Frequency)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 5.4 - Analysis for different values of $m$ and $R$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Set the values of m\n",
    "mag = [2,3,4,5]\n",
    "\n",
    "# Initialize r\n",
    "r = [0,0,0]\n",
    "\n",
    "# Inizialize vectors\n",
    "xx = np.empty((len(mag),len(r)), object)\n",
    "yy = np.empty((len(mag),len(r)), object)\n",
    "mean = np.empty((len(mag),len(r)))\n",
    "length = np.empty((len(mag),len(r)))\n",
    "\n",
    "# Create figure \n",
    "fig, axes = plt.subplots(nrows=len(mag), ncols=len(r), figsize=(20,20))\n",
    "fig1, axes1 = plt.subplots(nrows=len(mag), ncols=len(r), figsize=(20,20))\n",
    "  \n",
    "\n",
    "for i in range(len(mag)):    \n",
    "    df_red = copy.deepcopy(df[df['magnitude'] >= mag[i]])\n",
    "    df_red['distance'] = np.sqrt((df_red['X'].diff(periods=-1))**2 + (df_red['Y'].diff(periods=-1))**2 + \n",
    "                                 (df_red['Z'].diff(periods=-1))**2)     \n",
    "    df_red = df_red.dropna()  \n",
    "    r = df_red['distance'].mean()\n",
    "    r = [r/2,r,2*r]\n",
    "    \n",
    "    for j in range(len(r)):        \n",
    "        df_r = copy.deepcopy(df_red[df_red['distance']<r[j]])\n",
    "        df_r['waiting'] = df_r['time'].diff()\n",
    "        df_r.loc[df_r.idxmin(),['waiting']] = 0.\n",
    "        width = 1000*np.logspace(0, 3, int(np.sqrt(df_r.shape[0]))+1)\n",
    "        edge = np.cumsum(width)\n",
    "        title = \"m: {}; r:{}\".format(round(mag[i],2),round(r[j],2))\n",
    "        axes[i,j].set_title(title)    \n",
    "        axes[i,j].set_xlim(0,200000)\n",
    "        axes[i,j].set_xlabel('waiting')\n",
    "        axes[i,j].set_ylabel('frequency')\n",
    "        ax = sns.distplot(df_r['waiting'], bins = edge, kde=False, norm_hist=True, ax=axes[i,j])\n",
    "        edge = np.array([h.get_xy()[0] for h in ax.patches]) \n",
    "        width = np.array([h.get_width() for h in ax.patches])\n",
    "        xx[i,j] =  np.array(np.add(edge, -width/2.)) \n",
    "        yy[i,j] =  np.array([h.get_height() for h in ax.patches])\n",
    "        mean[i,j] = df_r['waiting'].mean()\n",
    "        length[i,j] = df_r['distance'].max() \n",
    "        plt.close()\n",
    "\n",
    "color = ['red','yellow','darkgreen','blue','grey','saddlebrown','lime','aqua','magenta','darkorange',\n",
    "         'dodgerblue','black']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "\n",
    "for i in range(len(mag)):\n",
    "    for j in range(len(r)):\n",
    "        xx[i,j] = xx[i,j]\n",
    "        yy[i,j] = yy[i,j]\n",
    "        \n",
    "        ax.plot(xx[i,j],yy[i,j],'.',label=\"m: {}; r: {}\".format(round(mag[i],2),round(r[j],0)),\n",
    "                c=color[3*i+j]) \n",
    "\n",
    "ax.legend()\n",
    "ax.set_title('Waiting time histograms for different filters for m and r')\n",
    "ax.set_xlabel('Waiting time [s]')\n",
    "ax.set_ylabel('Normalized frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 6 - Rescaling\n",
    "\n",
    "* A power law distribution is a scale-free distribution: $ p(bx) = g(b)p(x) $\n",
    "\n",
    "* Rescaling the data by a charateristic size they collapse into the same curve.\n",
    "\n",
    "* Rescaling is achived by: \n",
    "\n",
    "$$ x = \\frac{\\Delta d}{L} $$ \n",
    "\n",
    "$$ y = \\Delta d \\times L $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 6.1 - Rescaling $P_m(t)$\n",
    "\n",
    "We rescale waiting time distribution for different values of $m$ using as characteristic length the average waiting time.\n",
    "\n",
    "We observe that, by plotting average waiting time vs $m$, it turns out they are proportional as expected. Intuitively, the slope of the line they build up is related to the coefficient of proportionality.\n",
    "\n",
    "Morover we plot the dotted histogram with logarithminc binning for varius values of $m$ after rescaling and we observe that data collapses on the same curve as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Read data from file and make sure earthquakes are ordered by time\n",
    "cols = ['index', 'trigger', 'time', 'magnitude', 'X', 'Y', 'Z']\n",
    "df = pd.read_table(\"SouthCalifornia-1982-2011_Physics-of-Data.dat\",sep=\" \",header=None)\n",
    "df.columns = cols\n",
    "df = df.sort_values(by = ['time'])\n",
    "\n",
    "mag_interval = list(np.arange(df['magnitude'].min(),df['magnitude'].max()-2,0.1)) # Set the values of m\n",
    "waiting_mean = []\n",
    "\n",
    "for i in range(len(mag_interval)): # Compute the average waiting time after filtering for given m\n",
    "    df_red = copy.deepcopy(df[df['magnitude'] >= mag_interval[i]])\n",
    "    df_red['waiting'] = df_red['time'].diff()\n",
    "    df_red.loc[df_red.idxmin(),['waiting']] = 0.\n",
    "    waiting_mean.append(df_red['waiting'].mean())\n",
    "    \n",
    "fig = plt.subplots(figsize=(10,8))\n",
    "plt.plot(mag_interval,waiting_mean,'.')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.title('Average waiting time filtered by magnitude in log-log scale')\n",
    "plt.xlabel('Magnitude')\n",
    "plt.ylabel('Average waiting time [s]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Rescale waiting time multiplying frequency for average waiting time and dividing waiting time for average waiting time\n",
    "xx_waiting_res = [0,0,0,0]\n",
    "yy_waiting_res = [0,0,0,0]\n",
    "\n",
    "for i in range(len(mag)):\n",
    "    xx_waiting_res[i] = xx_waiting[i]/w_mean[i]\n",
    "    yy_waiting_res[i] = yy_waiting[i]*w_mean[i]\n",
    "\n",
    "fig, ax = plt.subplots(1,2,figsize=(16,8))\n",
    "\n",
    "fig.suptitle('Waiting time histograms for different minimum magnitude', fontsize=16)\n",
    "for i in range(len(mag)):\n",
    "    ax[0].plot(xx_waiting[i],yy_waiting[i],'.',label=\"m: {}\".format(i+2))\n",
    "    ax[0].set_xscale('log')\n",
    "    ax[0].set_yscale('log')\n",
    "    ax[0].set_title('Original')\n",
    "    ax[0].set_xlabel('Waiting time [s]')\n",
    "    ax[0].set_ylabel('Normalized frequency')\n",
    "    ax[0].legend()\n",
    "    \n",
    "for i in range(len(mag)):\n",
    "    ax[1].plot(xx_waiting_res[i],yy_waiting_res[i],'.',label=\"m: {}\".format(i+2))\n",
    "    ax[1].set_xscale('log')\n",
    "    ax[1].set_yscale('log')\n",
    "    ax[1].set_title('Rescaled')\n",
    "    ax[1].set_xlabel('Waiting time / mean waiting time')\n",
    "    ax[1].set_ylabel('Normalized frequency * mean waiting time')\n",
    "    ax[1].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 6.2 - Rescaling  $P_m(r)$\n",
    "\n",
    "We perform a similar procedure for distances using as characteristic length the maximum distance for each $m$. In this case we observe no significative rescaling. In fact observing the plot of maximum distances vs magnitude we notice that the coefficient of proportionality seems to be close to one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Read data from file and make sure earthquakes are ordered by time\n",
    "cols = ['index', 'trigger', 'time', 'magnitude', 'X', 'Y', 'Z']\n",
    "df = pd.read_table(\"SouthCalifornia-1982-2011_Physics-of-Data.dat\",sep=\" \",header=None)\n",
    "df.columns = cols\n",
    "df = df.sort_values(by = ['time'])\n",
    "\n",
    "mag_interval = list(np.arange(df['magnitude'].min(),df['magnitude'].max()-2,0.1)) # Set the values of m\n",
    "distance_max = []\n",
    "\n",
    "for i in range(len(mag_interval)): # Compute the maximum distance after filtering for given m\n",
    "    df_red = copy.deepcopy(df[df['magnitude'] >= mag_interval[i]])\n",
    "    df_red['distance'] = np.sqrt((df_red['X'].diff(periods=-1))**2 + (df_red['Y'].diff(periods=-1))**2 + \n",
    "                                 (df_red['Z'].diff(periods=-1))**2)\n",
    "    df_red = df_red.dropna()\n",
    "    distance_max.append(df_red['distance'].max())\n",
    "    \n",
    "fig = plt.subplots(figsize=(10,8))\n",
    "plt.plot(mag_interval,distance_max,'.')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.ylim(0,10000000)\n",
    "plt.title('Maximum distance filtered by magnitude in log-log scale')\n",
    "plt.xlabel('Magnitude')\n",
    "plt.ylabel('Maximum distance [m]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Rescale distance multiplying frequency for maximum distance and dividing distance for maximum distance\n",
    "xx_dist_res = [0,0,0,0]\n",
    "yy_dist_res = [0,0,0,0]\n",
    "\n",
    "for i in range(len(mag)):\n",
    "    xx_dist_res[i] = xx_dist[i]/max_dist[i]\n",
    "    yy_dist_res[i] = yy_dist[i]*max_dist[i]\n",
    "\n",
    "fig, ax = plt.subplots(1,2,figsize=(16,8))\n",
    "\n",
    "fig.suptitle('Distances histograms for different minimum magnitude', fontsize=16)\n",
    "for i in range(len(mag)):\n",
    "    ax[0].plot(xx_dist[i],yy_dist[i],'.',label=\"m: {}\".format(i+2))\n",
    "    ax[0].set_xscale('log')\n",
    "    ax[0].set_yscale('log')\n",
    "    ax[0].set_title('Original')\n",
    "    ax[0].set_xlabel('Distance [m]')\n",
    "    ax[0].set_ylabel('Normalized frequency')\n",
    "    ax[0].legend()\n",
    "    \n",
    "for i in range(len(mag)):\n",
    "    ax[1].plot(xx_dist_res[i],yy_dist_res[i],'.',label=\"m: {}\".format(i+2))\n",
    "    ax[1].set_xscale('log')\n",
    "    ax[1].set_yscale('log')\n",
    "    ax[1].set_title('Rescaled')\n",
    "    ax[1].set_xlabel('Distance / maximum distance')\n",
    "    ax[1].set_ylabel('Normalized frequency * maximum distance')\n",
    "    ax[1].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 6.3 - Reascaling $P_{m,R}(t)$ \n",
    "\n",
    "Eventually we rescale waiting time between events with magnitude above $m$ and closer than $R$ using as characteristic length the product between avearage waiting time and maximum distance. We observe again the collapse of data on the same curve. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Rescale distance and waiting time\n",
    "xx_res = np.empty((len(mag),len(r)), object)\n",
    "yy_res = np.empty((len(mag),len(r)), object)\n",
    "\n",
    "color = ['red','yellow','darkgreen','blue','grey','saddlebrown','lime','aqua','magenta','darkorange',\n",
    "         'dodgerblue','black']\n",
    "\n",
    "for i in range(len(mag)):\n",
    "    for j in range(len(r)):  \n",
    "        xx_res[i,j] = xx[i,j]/(w_mean[i]*max_dist[i])\n",
    "        yy_res[i,j] = yy[i,j]*w_mean[i]*max_dist[i]\n",
    "        \n",
    "fig, ax = plt.subplots(1,2,figsize=(16,8))\n",
    "\n",
    "fig.suptitle('Waiting time histograms for different minimum magnitude and maximum distance', fontsize=16)\n",
    "for i in range(len(mag)):\n",
    "    for j in range(len(r)):\n",
    "        ax[0].plot(xx[i,j],yy[i,j],'.',label=\"m:{}; r:{}\".format(round(mag[i],2),round(r[j],2)),\n",
    "                   c=color[3*i+j] )\n",
    "        ax[0].set_xscale('log')\n",
    "        ax[0].set_yscale('log')\n",
    "        ax[0].set_title('Original')\n",
    "        ax[0].set_xlabel('Waiting time [s]')\n",
    "        ax[0].set_ylabel('Normalized frequency')\n",
    "        ax[0].legend()\n",
    "    \n",
    "for i in range(len(mag)):\n",
    "    for j in range(len(r)):\n",
    "        ax[1].plot(xx_res[i,j],yy_res[i,j],'.',label=\"m:{}; r:{}\".format(round(mag[i],2),round(r[j],2)),\n",
    "                   c=color[3*i+j] )\n",
    "        ax[1].set_xscale('log')\n",
    "        ax[1].set_yscale('log')\n",
    "        ax[1].set_title('Rescaled')\n",
    "        ax[1].set_xlabel('Waiting time / (mean waiting time * maximum distance)')\n",
    "        ax[1].set_ylabel('Normalized frequency * mean waiting time * maximum distance')\n",
    "        ax[1].legend()\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this project we analized data of earthquakes in South California in a thirty year time span  and we observe a power law dacay for both waiting time and distance distribution for events closer in time and space than a threshold. We also observe a rescaling behaviour as far as waiting time distribution is concerned, while in the case of distances the reascaling behaviour is not appreciable. "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
